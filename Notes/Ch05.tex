\chapter{误差反向传播法}
通过数值微分计算了神经网
络的权重参数的梯度（严格来说，是损失函数关于权重参数的梯度）。数值微
分虽然简单，也容易实现，但缺点是计算上比较费时间。本章我们将学习一
个能够高效计算权重参数的梯度的方法——误差反向传播法。
\section{计算图}
计算图将计算过程用图形表示出来。这里说的图形是数据结构图，通
过多个节点和边表示（连接节点的直线称为“边”）。

计算图通过节点和箭头表示计算过程。节点用$\bigcirc$表示，$\bigcirc$中是计算的内
容。将计算的中间结果写在箭头的上方，表示各个节点的计算结果从左向右
传递。

\figures{Based on the calculation graph to solve the answer}

虽然\autoref{Based on the calculation graph to solve the answer}中把“$\times 2$”“$\times 1.1$”等作为一个运算整体用○括起来了，不过
只用$\bigcirc$表示乘法运算“$\times$”也是可行的。此时，如\autoref{Based on the calculation graph to solve the answer2}所示，可以将“2”和
“1.1”分别作为变量“苹果的个数”和“消费税”标在$\bigcirc$外面。

\figures{Based on the calculation graph to solve the answer2}

用计算图解题的情况下，需要按如下流程进行。
\begin{enumerate}
    \item 构建计算图。
    \item 在计算图上，从左向右进行计算。
\end{enumerate}

这里的第2歩“从左向右进行计算”是一种正方向上的传播，简称为\textbf{正
    向传播}（forward propagation）。正向传播是从计算图出发点到结束点的传播。
既然有正向传播这个名称，当然也可以考虑反向（从图上看的话，就是从右向左）
的传播。实际上，这种传播称为\textbf{反向传播}（backward propagation）。反向传
播将在接下来的导数计算中发挥重要作用。

\subsection{局部计算}
计算图的特征是可以通过传递“局部计算”获得最终结果。“局部”这个
词的意思是“与自己相关的某个小范围”。局部计算是指，无论全局发生了什么，
都能只根据与自己相关的信息输出接下来的结果。

\section{链式法则}
前面介绍的计算图的正向传播将计算结果正向（从左到右）传递，其计
算过程是我们日常接触的计算过程，所以感觉上可能比较自然。而反向传播
将局部导数向正方向的反方向（从右到左）传递，一开始可能会让人感到困惑。
传递这个局部导数的原理，是基于链式法则（chain rule）的。
\subsection{计算图的反向传播}

\subsection{什么是链式法则}
介绍链式法则时，我们需要先从复合函数说起。复合函数是由多个函数
构成的函数。比如，$z = (x + y)^2$是下面所示的两个式子构成的：
\begin{equation}
    \label{eq5-1}
    \begin{aligned}
        z & =t^2 \\
        t & =x+y \\
    \end{aligned}
\end{equation}

链式法则是关于复合函数的导数的性质，定义如下：
\begin{tcolorbox}
    如果某个函数由复合函数表示，则该复合函数的导数可以用构成复
    合函数的各个函数的导数的乘积表示。
\end{tcolorbox}

以\autoref{eq5-1}为例，$\frac{\partial z}{\partial x}$可以用$\frac{\partial z}{\partial t}$和$\frac{\partial t}{\partial x}$的乘积表示，可以写成下式：
\begin{equation}
    \frac{\partial z}{\partial x}=\frac{\partial z}{\partial t}\frac{\partial t}{\partial x}
\end{equation}

对于$z = (x + y)^2$，那么就有
\begin{equation*}
    \frac{\partial z}{\partial x}=\frac{\partial z}{\partial t}\frac{\partial t}{\partial x}=2t*1=2(x+y)
\end{equation*}
\subsection{链式法则和计算图}
如图所示，计算图的反向传播从右到左传播信号。反向传播的计算顺序
是，先将节点的输入信号乘以节点的局部导数（偏导数），然后再传递给下一
个节点。比如，反向传播时，“$**2$”节点的输入是$\frac{\partial z}{\partial z}$，将其乘以局部导数$\frac{\partial z}{\partial t}$（因为正向传播时输入是$t$、输出是$z$，所以这个节点的局部导数是$\frac{\partial z}{\partial t}$），然后传递给下一个节点。另外，图5-7中反向传播最开始的信号$\frac{\partial z}{\partial t}$在前面的数学式中没有出现，这是因为$\frac{\partial z}{\partial t}=1$，所以在刚才的式子中被省略了。

\section{反向传播}
上一节介绍了计算图的反向传播是基于链式法则成立的。本节将以“$+$”
和“$\times$”等运算为例，介绍反向传播的结构。
\subsection{加法节点的反向传播}
考虑加法节点的反向传播。这里以$z = x + y$为对象，观察它的
反向传播。$z = x + y$的导数可由下式（解析性地）计算出来：
\begin{equation*}
    \begin{aligned}
        \frac{\partial z}{\partial x} & =1 \\
        \frac{\partial z}{\partial y} & =1 \\
    \end{aligned}
\end{equation*}

\autoref{Backpropagation for Adder nodes}, 反向传播将从上游传过来的导数乘以1，然
后传向下游。也就是说，因为加法节点的反向传播只乘以1，所以输入的值
会原封不动地流向下一个节点。

\figures{Backpropagation for Adder nodes}

\subsection{乘法节点的反向传播}
看一下乘法节点的反向传播。这里我们考虑$z = xy$。这个
式子的导数用下式表示：
\begin{equation*}
    \begin{aligned}
        \frac{\partial z}{\partial x} & =y \\
        \frac{\partial z}{\partial y} & =x \\
    \end{aligned}
\end{equation*}
乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”
后传递给下游。
\figures{Backpropagation of multiply nodes}

加法的反向传播只是将上游的值传给下游，
并不需要正向传播的输入信号。但是，乘法的反向传播需要正向传播时的输
入信号值。因此，\important{实现乘法节点的反向传播时，要保存正向传播的输入信号}。

\figures{Example of backpropagation for buying apples}

\section{简单层的实现}
我们把要实现
的计算图的乘法节点称为“乘法层”
（MulLayer），加法节点称为“加法层”
（AddLayer）。
\subsection{乘法层的实现}
层的实现中有两个共通的方法（接口）forward()和backward()。forward()
对应正向传播，backward()对应反向传播。

\section{激活函数层的实现}
现在，我们将计算图的思路应用到神经网络中。这里，我们把构成神经
网络的层实现为一个类。先来实现激活函数的 ReLU层和 Sigmoid层。
\subsection{ReLU层}
激活函数ReLU（Rectified Linear Unit）由下式表示：
\begin{equation*}
    y = \left\{
    \begin{array}{ll}
        x, & x>0     \\
        0, & x\leq 0 \\
    \end{array}
    \right.
\end{equation*}

可以求出y关于x的导数，如下式所示：
\begin{equation}
    \label{eq5-8}
    \frac{\partial y}{\partial x} = \left\{
    \begin{array}{ll}
        1, & x>0     \\
        0, & x\leq 0 \\
    \end{array}
    \right.
\end{equation}

\autoref{eq5-8}中，如果正向传播时的输入$x$大于0，则反向传播会将上游的
值原封不动地传给下游。反过来，如果正向传播时的$x$小于等于0，则反向
传播中传给下游的信号将停在此处。

\begin{tcolorbox}
    ReLU层的作用就像电路中的开关一样。正向传播时，有电流通过
    的话，就将开关设为ON；没有电流通过的话，就将开关设为OFF。
    反向传播时，开关为ON的话，电流会直接通过；开关为OFF的话，
    则不会有电流通过。
\end{tcolorbox}

\subsection{Sigmoid层}
sigmoid函数由下式表示：
\begin{equation*}
    y=\frac{1}{1+\exp(-x)}
\end{equation*}

\section{Affine/Softmax层的实现}
\subsection{Affine层}
神经元的加权和可以用 \verb|Y = np.dot(X, W) + B|计算出来。然后，\verb|Y| 经过
激活函数转换后，传递给下一层。这就是神经网络正向传播的流程。
\begin{tcolorbox}
    神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿
    射变换”\footnote{几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。}。因此，这里将进行仿射变换的处理实现为“Affine层”。
\end{tcolorbox}