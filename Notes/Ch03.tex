\chapter{神经网络}
关于感知机，既有好消息，也有坏消息。好
消息是，即便对于复杂的函数，感知机也隐含着能够表示它的可能性。上一
章已经介绍过，即便是计算机进行的复杂处理，感知机（理论上）也可以将
其表示出来。坏消息是，设定权重的工作，即确定合适的、能符合预期的输
入与输出的权重，现在还是由人工进行的。
\section{从感知机到神经网络}
\subsection{神经网络的例子}
用图来表示神经网络的话，如\autoref{Examples of Neural Networks}所示。我们把最左边的一列称为
输入层，最右边的一列称为输出层，中间的一列称为中间层。中间层有时也称为隐藏层。“隐藏”一词的意思是，隐藏层的神经元（和输入层、输出
层不同）肉眼看不见。另外，本书中把输入层到输出层依次称为第 0层、第
1 层、第2 层。

\figures{Examples of Neural Networks}
\subsection{复习感知机}
引入新函数$h(x)$，将\autoref{eq2-2}改写成下面的方程：
\begin{equation}
    y=h(b+w_1x_1+w_2x_2)
\end{equation}
\begin{equation}
    \label{eq3-3}
    h(x)=\left\{
    \begin{array}{ll}
        0 & ,x\leq 0 \\
        1 & ,x>0
    \end{array}
    \right.
\end{equation}
\subsection{激活函数登场}
\autoref{eq3-3}中的$h(x)$函数会将输入信号的总和转换为输出信号，这种函数
一般称为激活函数（activation function）。
\begin{tcolorbox}
    本书在使用“感知机”一词时，没有严格统一它所指的算法。一
    般而言，“朴素感知机”是指单层网络，指的是激活函数使用了阶
    跃函数A 的模型。“多层感知机”是指神经网络，即使用sigmoid
    函数等平滑的激活函数的多层网络。
\end{tcolorbox}

\section{激活函数}
\autoref{eq3-3}表示的激活函数以阈值为界，一旦输入超过阈值，就切换输出。
这样的函数称为“阶跃函数\footnote{阶跃函数是指一旦输入超过阈值，就切换输出的函数。应该有更严格的数学定义}”。因此，可以说感知机中使用了阶跃函数作为
激活函数。也就是说，在激活函数的众多候选函数中，感知机使用了阶跃函数。
\subsection{sigmoid函数}
神经网络中经常使用的一个激活函数是sigmoid函数（sigmoid function）:
\begin{equation}
    h(x)=\frac{1}{1+\exp(-x)}
\end{equation}
\subsection{sigmoid函数和阶跃函数的比较}
首先注意到的是“平滑性”的不同。sigmoid函数是一条平
滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以0为界，输出发
生急剧性的变化。sigmoid函数的平滑性对神经网络的学习具有重要意义。

另一个不同点是，相对于阶跃函数只能返回0或1，sigmoid函数可以返
回0.731 . . .、0.880 . . .等实数（这一点和刚才的平滑性有关）。也就是说，感
知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续
的实数值信号。

接着说一下阶跃函数和sigmoid函数的共同性质。阶跃函数和sigmoid
函数虽然在平滑性上有差异，可以发现它们
具有相似的形状。实际上，两者的结构均是“输入小时，输出接近0（为0）；
随着输入增大，输出向1靠近（变成1）”。也就是说，当输入信号为重要信息时，
阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，
两者都输出较小的值。还有一个共同点是，不管输入信号有多小，或者有多
大，输出信号的值都在0到1之间。

\subsection{非线性函数}
神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使
用线性函数。为什么不能使用线性函数呢？因为使用线性函数的话，加深神
经网络的层数就没有意义了。

线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无
隐藏层的神经网络”。为了具体地（稍微直观地）理解这一点，我们来思
考下面这个简单的例子。这里我们考虑把线性函数 h(x) = cx 作为激活
函数，把 $y(x) = h(h(h(x)))$ 的运算对应 3 层神经网络\footnote{该对应只是一个近似，实际的神经网络运算比这个例子要复杂，但不影响后面的结论成立。}。这个运算会进行
$y(x) = c \times c \times c \times x$的乘法运算，但是同样的处理可以由$y(x) = ax$（注意，
$a = c^3$）这一次乘法运算（即没有隐藏层的神经网络）来表示。如本例所示，
使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所
带来的优势，激活函数必须使用非线性函数。

\subsection{ReLU函数}
在神经网络发展的历史上，sigmoid函数很早就开始被使用了，而最近则主要
使用ReLU（Rectified Linear Unit）函数。ReLU函数可以表示为:
\begin{equation}
    h(x)=\left\{
    \begin{array}{ll}
        x & , x>0     \\
        0 & , x\leq 0 \\
    \end{array}
    \right.
\end{equation}
\section{多维数组的运算}
\verb|np.dot()|接收两个NumPy数组作为参数，并返回数组的乘积。

\section{输出层的设计}
神经网络可以用在分类问题和回归问题上，不过需要根据情况改变输出
层的激活函数。一般而言，回归问题用恒等函数，分类问题用softmax函数。
\subsection{恒等函数和softmax函数}
恒等函数会将输入按原样输出，对于输入的信息，不加以任何改动地直
接输出。因此，在输出层使用恒等函数时，输入信号会原封不动地被输出。和前面介绍的隐藏层的激活函数一样，恒等函数进行的转换处理可以
用一根箭头来表示。

分类问题中使用的softmax函数表示为：
\begin{equation}
    \label{eq3-10}
    y_k=\frac{\exp(a_k)}{\sum\limits_{i=1}^n{\exp(a_i)}}
\end{equation}

softmax函数的分子是输入信号$a_k$的指数函数，分母是所有输入信号的指数
函数的和。

\subsection{实现softmax函数时的注意事项}
\autoref{eq3-10}在计算机的运算
上有一定的缺陷。这个缺陷就是溢出问题。softmax函数的实现中要进行指
数函数的运算，但是此时指数函数的值很容易变得非常大。

\begin{equation}
    \begin{aligned}
        y_k=\frac{\exp(a_k)}{\sum\limits_{i=1}^n{\exp(a_i)}} & =\frac{C\exp(a_k)}{C\sum\limits_{i=1}^n{\exp(a_i)}}             \\
                                                             & =\frac{\exp(a_k+\log C)}{\sum\limits_{i=1}^n{\exp(a_i+\log C)}} \\
                                                             & =\frac{\exp(a_k+C^{'})}{\sum\limits_{i=1}^n{\exp(a_i+C^{'})}}   \\
    \end{aligned}
\end{equation}

在进行softmax的指数函数的运算时，加上（或者减去）
某个常数并不会改变运算的结果。这里的$C^{'}$可以使用任何值，但是为了防
止溢出，一般会使用输入信号中的最大值。
\subsection{softmax函数的特征}
一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。
并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，
神经网络在进行分类时，输出层的softmax函数可以省略。在实际的问题中，
由于指数函数的运算需要一定的计算机运算量，因此输出层的softmax函数
一般会被省略。
\subsection{输出层的神经元数量}
